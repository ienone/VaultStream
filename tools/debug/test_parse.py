import os
import sys
import json
import asyncio
import re
import time
from datetime import datetime
from pathlib import Path
from dotenv import load_dotenv

# æ·»åŠ é¡¹ç›®è·¯å¾„ä»¥å¯¼å…¥ app æ¨¡å—
sys.path.insert(0, os.path.dirname(__file__))

# Windows å¿…é¡»åœ¨å¯¼å…¥ crawl4ai ä¹‹å‰è®¾ç½®
if sys.platform == 'win32':
    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv(os.path.join(os.path.dirname(__file__), '.env'))

from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator
from crawl4ai.content_filter_strategy import PruningContentFilter

from app.core.crawler_config import get_delay_for_url_sync

# ========== é…ç½® ==========
TEST_URL = "https://blog.ienone.top/anime/anime-review-2025-07/"
# TEST_URL = "https://x.com/PokeMikuVOLTAGE/status/2006379887943434462"
# TEST_URL= "https://www.bilibili.com/opus/1150580721704763430/?from=readlist"

# è¾“å‡ºç›®å½•
OUTPUT_DIR = Path(__file__).parent / "test_output"

# æ˜¯å¦å¯ç”¨ LLM å…ƒæ•°æ®æå–
ENABLE_LLM_EXTRACTION = True

# LLM å…ƒæ•°æ®æå– Schemaï¼ˆä¸åŒ…å«æ­£æ–‡å’Œé“¾æ¥ï¼Œè¿™äº›ä» Markdown è·å–ï¼‰
METADATA_SCHEMA = {
    "type": "object",
    "properties": {
        "title": {"type": "string", "description": "The main title of the article or post."},
        "author": {"type": "string", "description": "Name of the author or publisher."},
        "summary": {"type": "string", "description": "A concise summary of the content (2-3 sentences)."},
        "publish_date": {"type": "string", "description": "Publication date if available (YYYY-MM-DD HH:MM format preferred)."},
        "tags": {"type": "array", "items": {"type": "string"}, "description": "Relevant topics or tags."},
        "detected_type": {
            "type": "string",
            "enum": ["article", "video", "gallery", "audio"],
            "description": "Detected content type: 'article' for long-form text, 'video' for video content, 'gallery' for image-heavy posts, 'audio' for podcasts."
        },
        "metrics": {
            "type": "object",
            "properties": {
                "view_count": {"type": "integer", "description": "Number of views/reads."},
                "like_count": {"type": "integer", "description": "Number of likes/hearts."},
                "comment_count": {"type": "integer", "description": "Number of comments/replies."},
                "share_count": {"type": "integer", "description": "Number of shares/retweets."}
            }
        }
    },
    "required": ["title"]
}

# LLM æå–æç¤ºè¯
EXTRACTION_PROMPT = """
Analyze the web page content and extract metadata.

IMPORTANT:
1. Extract the title, author, and publish date if visible.
2. Look for interaction metrics (views, likes, comments, shares) - often found near the post header or footer.
3. Generate a brief 2-3 sentence summary of the main content.
4. Detect the content type based on what the page primarily shows.
5. Extract relevant tags or topics.

Do NOT extract the full content text - only metadata.
Return valid JSON matching the schema.
"""


def save_result(filename: str, content: str):
    """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    filepath = OUTPUT_DIR / filename
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(content)
    print(f"âœ“ å·²ä¿å­˜: {filepath}")


def get_llm_config():
    """è·å– LLM é…ç½®"""
    api_key = os.getenv("TEXT_LLM_API_KEY")
    base_url = os.getenv("TEXT_LLM_BASE_URL")
    model = os.getenv("TEXT_LLM_MODEL", "deepseek-chat")
    
    if not api_key:
        print("[LLM] è­¦å‘Š: TEXT_LLM_API_KEY æœªé…ç½®ï¼Œè·³è¿‡ LLM æå–")
        return None
    
    return {
        "provider": f"openai/{model}",
        "api_token": api_key,
        "base_url": base_url
    }


async def extract_metadata_with_llm(markdown_content: str, llm_config: dict) -> dict:
    """
    ä½¿ç”¨ LLM ä» Markdown å†…å®¹ä¸­æå–å…ƒæ•°æ®
    """
    from langchain_openai import ChatOpenAI
    
    print("\n[LLM] å¼€å§‹å…ƒæ•°æ®æå–...")
    start_time = time.time()
    
    try:
        llm = ChatOpenAI(
            model=llm_config["provider"].split("/")[-1],  # æå–æ¨¡å‹å
            api_key=llm_config["api_token"],
            base_url=llm_config["base_url"],
            temperature=0.1,
        )
        
        # æ„å»ºæç¤º
        prompt = f"""{EXTRACTION_PROMPT}

Schema:
```json
{json.dumps(METADATA_SCHEMA, indent=2)}
```

Content to analyze:
```
{markdown_content[:8000]}  
```

Respond with valid JSON only, no markdown code blocks:"""

        response = await llm.ainvoke(prompt)
        response_text = response.content.strip()
        
        # å°è¯•æ¸…ç†å’Œè§£æ JSON
        if response_text.startswith("```"):
            # ç§»é™¤ markdown ä»£ç å—
            response_text = re.sub(r'^```(?:json)?\n?', '', response_text)
            response_text = re.sub(r'\n?```$', '', response_text)
        
        metadata = json.loads(response_text)
        
        elapsed = time.time() - start_time
        print(f"[LLM] æå–å®Œæˆ (è€—æ—¶: {elapsed:.2f}s)")
        
        return metadata
        
    except json.JSONDecodeError as e:
        print(f"[LLM] JSON è§£æå¤±è´¥: {e}")
        print(f"[LLM] åŸå§‹å“åº”: {response_text[:500]}...")
        return {}
    except Exception as e:
        print(f"[LLM] æå–å¤±è´¥: {type(e).__name__}: {e}")
        return {}


async def crawl_and_extract():
    print(f"=" * 60)
    print(f"æµ‹è¯• URL: {TEST_URL}")
    print(f"è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    print(f"LLM æå–: {'å¯ç”¨' if ENABLE_LLM_EXTRACTION else 'ç¦ç”¨'}")
    print(f"=" * 60)
    
    # æµè§ˆå™¨é…ç½® - å¢åŠ ä¼ªè£…ä»¥é¿å… JavaScript æ£€æµ‹
    browser_config = BrowserConfig(
        headless=True,
        verbose=True,
        user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
    )
    
    # å†…å®¹è¿‡æ»¤å™¨ - ä½¿ç”¨å›ºå®šé˜ˆå€¼é¿å…ä¸ç¨³å®šæ€§
    content_filter = PruningContentFilter(
        threshold=0.45,
        threshold_type="fixed",  # å›ºå®šé˜ˆå€¼
        min_word_threshold=0,
    )
    
    # Markdown ç”Ÿæˆå™¨
    md_generator = DefaultMarkdownGenerator(
        content_filter=content_filter,
        options={
            "ignore_links": False,
            "ignore_images": False,
            "escape_html": True,
            "body_width": 0,
            "skip_internal_links": False,
        }
    )
    
    # ========== æ ¸å¿ƒè¿‡æ»¤é…ç½® ==========
    excluded_tags = [
        "nav", "header", "footer", "aside",
        "form", "iframe", "noscript", "script", "style",
        "svg", "canvas",
    ]
    
    excluded_selector = ",".join([
        ".navbar", ".nav", ".navigation", ".menu", ".breadcrumb",
        "[role='navigation']", "[role='banner']",
        ".sidebar", ".toc", ".table-of-contents",
        ".header", ".footer", "[role='contentinfo']",
        ".comments", ".comment-section", ".social-share", ".share-buttons",
        ".ad", ".ads", ".advertisement", ".advert",
        ".widget", ".popup", ".modal", ".cookie-notice",
        ".subscribe", ".newsletter", ".related-posts",
    ])
    
    excluded_domains = [
        "facebook.com", "instagram.com",
        "linkedin.com", "pinterest.com", "tiktok.com",
        "youtube.com", "reddit.com", "discord.com",
    ]
    
    # æ ¹æ® URL è‡ªåŠ¨è·å–ç­‰å¾…æ—¶é—´
    delay_time = get_delay_for_url_sync(TEST_URL)
    print(f"[é…ç½®] æ ¹æ®åŸŸåè‡ªåŠ¨è®¾ç½®ç­‰å¾…æ—¶é—´: {delay_time}s")
    
    # çˆ¬å–é…ç½®
    run_config = CrawlerRunConfig(
        cache_mode=CacheMode.BYPASS,
        wait_for="body",
        delay_before_return_html=delay_time,
        
        markdown_generator=md_generator,
        
        word_count_threshold=5,
        excluded_tags=excluded_tags,
        excluded_selector=excluded_selector,
        
        exclude_external_links=False,
        exclude_social_media_links=False,
        exclude_domains=excluded_domains,
        
        exclude_external_images=False,
        
        process_iframes=False,
        remove_overlay_elements=False,  # å¿…é¡»ç¦ç”¨ï¼ä¼šè¯¯åˆ  SPA é¡µé¢å†…å®¹
        magic=True,
    )
    
    print(f"\nå·²é…ç½®çš„è¿‡æ»¤è§„åˆ™:")
    print(f"  - excluded_tags: {len(excluded_tags)} ä¸ªæ ‡ç­¾")
    print(f"  - excluded_selector: {len(excluded_selector.split(','))} ä¸ªé€‰æ‹©å™¨")
    
    print("\nå¼€å§‹çˆ¬å–...")
    start_time = time.time()
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    try:
        async with AsyncWebCrawler(config=browser_config) as crawler:
            result = await crawler.arun(url=TEST_URL, config=run_config)
            
            crawl_elapsed = time.time() - start_time
            
            # ä¿å­˜åŸå§‹ HTMLï¼ˆè°ƒè¯•ç”¨ï¼‰
            if result.html:
                save_result(f"{timestamp}_debug.html", result.html)
                print(f"[DEBUG] HTML é•¿åº¦: {len(result.html)} å­—ç¬¦")
            else:
                print("[DEBUG] è­¦å‘Š: HTML ä¸ºç©º!")
            
            if result.cleaned_html:
                print(f"[DEBUG] Cleaned HTML é•¿åº¦: {len(result.cleaned_html)} å­—ç¬¦")
            
            # è·å– Markdown
            if hasattr(result.markdown, 'raw_markdown'):
                raw_markdown = result.markdown.raw_markdown
                fit_markdown = result.markdown.fit_markdown
            else:
                raw_markdown = str(result.markdown) if result.markdown else ""
                fit_markdown = None
            
            print(f"[DEBUG] Markdown é•¿åº¦: {len(raw_markdown)} å­—ç¬¦")
            
            # ä¿å­˜ Markdown
            if raw_markdown:
                save_result(f"{timestamp}_content.md", raw_markdown)
            else:
                print("[DEBUG] è­¦å‘Š: Markdown ä¸ºç©º!")
            
            if fit_markdown and fit_markdown != raw_markdown:
                save_result(f"{timestamp}_fit.md", fit_markdown)
            
            # æå–å›¾ç‰‡åˆ—è¡¨
            images = re.findall(r'!\[.*?\]\((.*?)\)', raw_markdown)
            
            # ========== LLM å…ƒæ•°æ®æå– ==========
            metadata = {}
            if ENABLE_LLM_EXTRACTION and raw_markdown:
                llm_config = get_llm_config()
                if llm_config:
                    # ä½¿ç”¨ fit_markdownï¼ˆæ›´å¹²å‡€ï¼‰æˆ– raw_markdown
                    content_for_llm = fit_markdown if fit_markdown else raw_markdown
                    metadata = await extract_metadata_with_llm(content_for_llm, llm_config)
            
            # æ„å»ºæœ€ç»ˆç»“æœ
            total_elapsed = time.time() - start_time
            
            final_result = {
                "url": TEST_URL,
                "content": fit_markdown or raw_markdown,
                "raw_content": raw_markdown,
                "images": images[:30],
                "metadata": metadata,
                "stats": {
                    "content_length": len(raw_markdown),
                    "fit_content_length": len(fit_markdown) if fit_markdown else 0,
                    "image_count": len(images),
                    "crawl_time": round(crawl_elapsed, 2),
                    "total_time": round(total_elapsed, 2),
                    "llm_extracted": bool(metadata),
                }
            }
            
            save_result(f"{timestamp}_result.json", json.dumps(final_result, ensure_ascii=False, indent=2))
            
            # æ‰“å°æ‘˜è¦
            print(f"\n{'=' * 60}")
            print(f"=== ç»“æœæ‘˜è¦ (æ€»è€—æ—¶: {total_elapsed:.2f}s) ===")
            print(f"{'=' * 60}")
            print(f"æˆåŠŸ: {result.success}")
            print(f"çŠ¶æ€ç : {result.status_code}")
            print(f"Markdown é•¿åº¦: {len(raw_markdown)} å­—ç¬¦")
            print(f"Fit Markdown é•¿åº¦: {len(fit_markdown) if fit_markdown else 0} å­—ç¬¦")
            print(f"æå–å›¾ç‰‡æ•°: {len(images)}")
            
            if metadata:
                print(f"\n=== LLM æå–çš„å…ƒæ•°æ® ===")
                print(f"æ ‡é¢˜: {metadata.get('title', 'N/A')}")
                print(f"ä½œè€…: {metadata.get('author', 'N/A')}")
                print(f"å‘å¸ƒæ—¥æœŸ: {metadata.get('publish_date', 'N/A')}")
                print(f"å†…å®¹ç±»å‹: {metadata.get('detected_type', 'N/A')}")
                print(f"æ‘˜è¦: {metadata.get('summary', 'N/A')[:100]}...")
                if metadata.get('metrics'):
                    m = metadata['metrics']
                    print(f"äº’åŠ¨æ•°æ®: ğŸ‘ {m.get('view_count', 0)} | â¤ {m.get('like_count', 0)} | ğŸ’¬ {m.get('comment_count', 0)} | ğŸ”„ {m.get('share_count', 0)}")
                if metadata.get('tags'):
                    print(f"æ ‡ç­¾: {', '.join(metadata['tags'][:5])}")
            
            # é¢„è§ˆå†…å®¹
            print(f"\n=== å†…å®¹é¢„è§ˆ ===")
            preview_content = fit_markdown if fit_markdown else raw_markdown
            print(preview_content[:500])
            print("...")
            
            print(f"\næ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {OUTPUT_DIR}")
                
    except Exception as e:
        print(f"\né”™è¯¯: {type(e).__name__}: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(crawl_and_extract())
