"""æµ‹è¯•è„šæœ¬ - æµ‹è¯•Bç«™é€‚é…å™¨å’Œå®Œæ•´æµç¨‹"""

import os
import sys


# Make `import app.*` work when running this file from `tests/`.
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)


import asyncio
import hashlib
import json
import shutil
import subprocess

from app.adapters.bilibili import BilibiliAdapter


async def test_bilibili_adapter():
    """æµ‹è¯•Bç«™é€‚é…å™¨"""
    adapter = BilibiliAdapter()

    # æµ‹è¯•URLåˆ—è¡¨
    test_urls = [
        "https://www.bilibili.com/video/BV1xx411c7XD",  # è§†é¢‘
        "https://www.bilibili.com/read/cv12345678",  # ä¸“æ 
        "https://www.bilibili.com/opus/1150580721704763430",  # åŠ¨æ€
    ]

    for url in test_urls:
        print(f"\n{'='*60}")
        print(f"æµ‹è¯•URL: {url}")
        print(f"{'='*60}")

        try:
            # æ£€æµ‹å†…å®¹ç±»å‹
            content_type = await adapter.detect_content_type(url)
            print(f"âœ… å†…å®¹ç±»å‹: {content_type}")

            # å‡€åŒ–URL
            clean_url = await adapter.clean_url(url)
            print(f"âœ… å‡€åŒ–URL: {clean_url}")

            # è§£æå†…å®¹ï¼ˆå¯èƒ½ä¼šå¤±è´¥ï¼Œå› ä¸ºIDæ˜¯ç¤ºä¾‹ï¼‰
            try:
                parsed = await adapter.parse(url)
                print(f"âœ… æ ‡é¢˜: {parsed.title}")
                print(f"âœ… ä½œè€…: {parsed.author_name}")
                print(f"âœ… æè¿°: {parsed.description[:100] if parsed.description else 'N/A'}...")
                print(f"âœ… å°é¢: {parsed.cover_url}")
            except Exception as e:
                print(f"âš ï¸  è§£æå¤±è´¥ï¼ˆé¢„æœŸï¼Œå› ä¸ºæ˜¯ç¤ºä¾‹IDï¼‰: {e}")

        except Exception as e:
            print(f"âŒ é”™è¯¯: {e}")


async def test_real_url():
    """æµ‹è¯•çœŸå®URLï¼ˆéœ€è¦æ‰‹åŠ¨æä¾›ï¼‰"""
    adapter = BilibiliAdapter()

    # è¿™é‡Œæ”¾ä¸€ä¸ªçœŸå®çš„Bç«™URL
    try:
        real_url = input("\nè¯·è¾“å…¥ä¸€ä¸ªçœŸå®çš„Bç«™URLè¿›è¡Œæµ‹è¯•ï¼ˆç›´æ¥å›è½¦è·³è¿‡ï¼‰: ").strip()
    except EOFError:
        real_url = ""

    if not real_url:
        print("è·³è¿‡çœŸå®URLæµ‹è¯•")
        return

    try:
        print(f"\n{'='*60}")
        print(f"è§£æçœŸå®URL: {real_url}")
        print(f"{'='*60}")

        parsed = await adapter.parse(real_url)

        print(f"\nğŸ“Š è§£æç»“æœ:")
        print(f"  å¹³å°: {parsed.platform}")
        print(f"  ç±»å‹: {parsed.content_type}")
        print(f"  ID: {parsed.content_id}")
        print(f"  æ ‡é¢˜: {parsed.title}")
        print(f"  ä½œè€…: {parsed.author_name} (ID: {parsed.author_id})")
        print(f"  æè¿°: {parsed.description[:200] if parsed.description else 'N/A'}...")
        print(f"  å°é¢: {parsed.cover_url}")
        print(f"  åª’ä½“æ•°: {len(parsed.media_urls)}")
        print(f"\n  å…ƒæ•°æ®:")
        for key, value in parsed.raw_metadata.items():
            print(f"    {key}: {value}")

        print(f"\nâœ… è§£ææˆåŠŸï¼")

    except Exception as e:
        print(f"âŒ è§£æå¤±è´¥: {e}")
        import traceback

        traceback.print_exc()


async def test_opus_archive():
    """åŠŸèƒ½æµ‹è¯•ï¼šopus å›¾æ–‡å½’æ¡£æ¸…æ´—ã€‚

    ä½¿ç”¨æ–¹å¼ï¼š
    1) ç¯å¢ƒå˜é‡ï¼šBILIBILI_TEST_OPUS_URL='https://www.bilibili.com/opus/xxxx' ç›´æ¥è·‘
    2) æˆ–è¿è¡ŒåæŒ‰æç¤ºè¾“å…¥
    """

    adapter = BilibiliAdapter()

    url = (os.getenv("BILIBILI_TEST_OPUS_URL") or "").strip()
    if not url:
        try:
            url = input("\nè¯·è¾“å…¥ä¸€ä¸ª Bç«™ opus åŠ¨æ€ URLï¼ˆç”¨äºå½’æ¡£æµ‹è¯•ï¼Œå›è½¦è·³è¿‡ï¼‰: ").strip()
        except EOFError:
            url = ""
    if not url:
        print("è·³è¿‡ opus å½’æ¡£æµ‹è¯•")
        return

    parsed = await adapter.parse(url)
    archive = (parsed.raw_metadata or {}).get("archive") or {}

    print("\nğŸ“¦ Opus å½’æ¡£æ¸…æ´—ç»“æœ:")
    print(f"  æ ‡é¢˜: {archive.get('title')!r}")
    print(f"  plain_text_len: {len(str(archive.get('plain_text') or ''))}")
    print(f"  markdown_len: {len(str(archive.get('markdown') or ''))}")
    print(f"  blocks: {len(archive.get('blocks') or [])}")
    print(f"  images: {len(archive.get('images') or [])}")
    print(f"  links: {len(archive.get('links') or [])}")
    print(f"  mentions: {len(archive.get('mentions') or [])}")
    print(f"  topics: {len(archive.get('topics') or [])}")

    # é¢„è§ˆå‰ 200 å­—
    preview = str(archive.get("plain_text") or "")
    if preview:
        print("\n  plain_text_preview:")
        print("  " + preview[:200].replace("\n", "\\n") + ("..." if len(preview) > 200 else ""))


async def test_opus_archive_from_curl_fixture():
    """ç¦»çº¿åŠŸèƒ½æµ‹è¯•ï¼šä½¿ç”¨ curl_opus.txt çš„å“åº” JSON æ„å»ºå½’æ¡£ã€‚

    ç›®çš„ï¼šä¸ä¾èµ–ç½‘ç»œ/é£æ§/ç™»å½•ï¼Œç¡®ä¿ _build_opus_archive å¯¹ module_content.paragraphs ç»“æ„å¯ç”¨ã€‚
    """

    fixture_path = (os.getenv("BILIBILI_OPUS_CURL_FIXTURE") or "curl_opus.txt").strip()
    if not fixture_path:
        print("è·³è¿‡ç¦»çº¿ opus fixture æµ‹è¯•ï¼ˆæœªæä¾› fixture è·¯å¾„ï¼‰")
        return

    if not os.path.exists(fixture_path):
        print(f"è·³è¿‡ç¦»çº¿ opus fixture æµ‹è¯•ï¼ˆæ–‡ä»¶ä¸å­˜åœ¨ï¼‰: {fixture_path}")
        return

    raw = ""
    with open(fixture_path, "r", encoding="utf-8", errors="ignore") as f:
        raw = f.read()

    start = raw.find("{")
    end = raw.rfind("}")
    if start < 0 or end < 0 or end <= start:
        print(f"è·³è¿‡ç¦»çº¿ opus fixture æµ‹è¯•ï¼ˆæœªæ‰¾åˆ° JSONï¼‰: {fixture_path}")
        return

    payload = json.loads(raw[start : end + 1])
    item = (((payload or {}).get("data") or {}).get("item") or {})

    adapter = BilibiliAdapter()
    archive = adapter._build_opus_archive(item)

    print("\nğŸ§· ç¦»çº¿ Opus fixture å½’æ¡£æ¸…æ´—ç»“æœ:")
    print(f"  æ ‡é¢˜: {archive.get('title')!r}")
    print(f"  plain_text_len: {len(str(archive.get('plain_text') or ''))}")
    print(f"  markdown_len: {len(str(archive.get('markdown') or ''))}")
    print(f"  blocks: {len(archive.get('blocks') or [])}")
    print(f"  images: {len(archive.get('images') or [])}")
    print(f"  links: {len(archive.get('links') or [])}")

    preview = str(archive.get("plain_text") or "")
    if preview:
        print("\n  plain_text_preview:")
        print("  " + preview[:200].replace("\n", "\\n") + ("..." if len(preview) > 200 else ""))

    # å¯¼å‡ºï¼šMarkdown/JSONï¼ˆç”¨äºäººå·¥æ ¸å¯¹â€œä¿å­˜æ˜¯å¦å®Œå–„â€ï¼‰
    export_dir = (os.getenv("VS_EXPORT_DIR") or "exports").strip() or "exports"
    export_md = (os.getenv("VS_EXPORT_MARKDOWN") or "1").strip().lower() not in ("0", "false", "no")
    export_pdf = (os.getenv("VS_EXPORT_PDF") or "0").strip().lower() in ("1", "true", "yes")
    export_webp = (os.getenv("VS_EXPORT_WEBP") or "0").strip().lower() in ("1", "true", "yes")

    if export_md:
        os.makedirs(export_dir, exist_ok=True)
        base = "opus_fixture"
        md_path = os.path.join(export_dir, f"{base}.md")
        json_path = os.path.join(export_dir, f"{base}.archive.json")

        md_text = str(archive.get("markdown") or "")

        if export_webp:
            try:
                import httpx
            except Exception:
                httpx = None

            try:
                from PIL import Image  # type: ignore
            except Exception:
                Image = None

            if httpx is None or Image is None:
                print("\nâš ï¸  VS_EXPORT_WEBP=1 éœ€è¦é¢å¤–ä¾èµ–ï¼šPillowï¼ˆä»¥åŠç½‘ç»œä¸‹è½½èƒ½åŠ›ï¼‰ã€‚")
            else:
                assets_dir = os.path.join(export_dir, "assets")
                os.makedirs(assets_dir, exist_ok=True)

                url_to_rel: dict[str, str] = {}

                async with httpx.AsyncClient(timeout=30.0) as client:  # ä¸‹è½½å¹¶è½¬ç å›¾ç‰‡ä¸º WebP
                    for img in (archive.get("images") or []):
                        if not isinstance(img, dict):
                            continue
                        url = img.get("url")
                        if not isinstance(url, str) or not url.strip():
                            continue
                        url = url.strip()
                        if url in url_to_rel:
                            continue

                        digest = hashlib.sha1(url.encode("utf-8")).hexdigest()[:16]
                        filename = f"{digest}.webp"
                        out_path = os.path.join(assets_dir, filename)
                        rel_path = os.path.join("assets", filename).replace("\\", "/")  # å…¼å®¹ Windows è·¯å¾„

                        if os.path.exists(out_path):
                            url_to_rel[url] = rel_path
                            continue

                        try:
                            resp = await client.get(url)
                            resp.raise_for_status()
                            data = resp.content
                        except Exception as e:
                            print(f"\nâš ï¸  ä¸‹è½½å›¾ç‰‡å¤±è´¥ï¼Œè·³è¿‡ï¼š{url} ({type(e).__name__}: {e})")
                            continue

                        try:
                            from io import BytesIO

                            with Image.open(BytesIO(data)) as im:
                                # ç»Ÿä¸€è½¬ä¸ºå¯å†™ webp çš„æ¨¡å¼
                                if im.mode in ("P", "LA"):
                                    im = im.convert("RGBA")
                                elif im.mode not in ("RGB", "RGBA"):
                                    im = im.convert("RGB")

                                im.save(out_path, format="WEBP", quality=80, method=6)
                            url_to_rel[url] = rel_path
                        except Exception as e:
                            print(f"\nâš ï¸  è½¬ç  webp å¤±è´¥ï¼Œè·³è¿‡ï¼š{url} ({type(e).__name__}: {e})")
                            continue

                # æ›´æ–° Markdownï¼šæŠŠè¿œç¨‹å›¾ç‰‡é“¾æ¥æ›¿æ¢æˆæœ¬åœ° assets è·¯å¾„
                for src_url, rel in url_to_rel.items():
                    md_text = md_text.replace(f"]({src_url})", f"]({rel})")

                print(f"\nğŸ–¼ï¸  å·²å¯¼å‡ºå¹¶è½¬ç å›¾ç‰‡åˆ°ï¼š{assets_dir}")

        with open(md_path, "w", encoding="utf-8") as f:
            f.write(md_text)
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(archive, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ“ å·²å¯¼å‡º Markdownï¼š{md_path}")
        print(f"ğŸ—ƒï¸  å·²å¯¼å‡º Archive JSONï¼š{json_path}")

        if export_pdf:
            pandoc = shutil.which("pandoc")
            if not pandoc:
                print("\nâš ï¸  VS_EXPORT_PDF=1 éœ€è¦ç³»ç»Ÿå·²å®‰è£… pandocï¼Œå½“å‰æœªæ‰¾åˆ° pandocã€‚")
            else:
                pdf_path = os.path.join(export_dir, f"{base}.pdf")
                try:
                    # ä½¿ç”¨ pandoc å°† md è½¬ PDFï¼ˆåŒ…å«å›¾ç‰‡æ—¶ä¼šå¼•ç”¨æœ¬åœ° assetsï¼‰
                    subprocess.run([pandoc, md_path, "-o", pdf_path], check=False)
                    print(f"\nğŸ“„ å·²å°è¯•å¯¼å‡º PDFï¼š{pdf_path}")
                except Exception as e:
                    print(f"\nâš ï¸  å¯¼å‡º PDF å¤±è´¥ï¼š{type(e).__name__}: {e}")


async def test_archive_media_processing_local_from_fixture():
    """ç¦»çº¿éªŒè¯ï¼šå­˜å‚¨æŠ½è±¡ + å›¾ç‰‡è½¬ WebP è½åˆ° storageï¼ˆLocalFSï¼‰ã€‚

    ç›®çš„ï¼šéªŒè¯ app.media_processing.store_archive_images_as_webp + app.storage.LocalStorageBackend å¯ç”¨ï¼Œ
    å¹¶ä¸”ä¼šæŠŠç»“æœå†™å› archiveï¼ˆstored_key/stored_sha256/stored_images ç­‰ï¼‰ã€‚

    å¯ç”¨æ–¹å¼ï¼š
    - VS_TEST_STORAGE_MEDIA=1
    - å¯é€‰ï¼šVS_TEST_STORAGE_ROOT=exports/storage_test
    - å¯é€‰ï¼šVS_TEST_PUBLIC_BASE_URL=http://localhost:9000 ï¼ˆç”¨äºæ›¿æ¢ markdown ä¸­å›¾ç‰‡é“¾æ¥ï¼‰
    """

    enabled = (os.getenv("VS_TEST_STORAGE_MEDIA") or "0").strip().lower() in ("1", "true", "yes")
    if not enabled:
        return

    fixture_path = (os.getenv("BILIBILI_OPUS_CURL_FIXTURE") or "curl_opus.txt").strip()
    if not fixture_path or not os.path.exists(fixture_path):
        print(f"è·³è¿‡ storage/webp ç¦»çº¿éªŒè¯ï¼ˆfixture ä¸å­˜åœ¨ï¼‰: {fixture_path}")
        return

    raw = ""
    with open(fixture_path, "r", encoding="utf-8", errors="ignore") as f:
        raw = f.read()
    start = raw.find("{")
    end = raw.rfind("}")
    if start < 0 or end < 0 or end <= start:
        print(f"è·³è¿‡ storage/webp ç¦»çº¿éªŒè¯ï¼ˆæœªæ‰¾åˆ° JSONï¼‰: {fixture_path}")
        return

    payload = json.loads(raw[start : end + 1])
    item = (((payload or {}).get("data") or {}).get("item") or {})

    adapter = BilibiliAdapter()
    archive = adapter._build_opus_archive(item)

    images = archive.get("images") or []
    print("\nğŸ§ª ç¦»çº¿éªŒè¯ï¼šstorage + WebP")
    print(f"  fixture_images: {len(images)}")
    if not images:
        print("  âš ï¸  fixture ä¸åŒ…å«å›¾ç‰‡ï¼Œæ— æ³•éªŒè¯è½¬ç /å­˜å‚¨")
        return

    # Local storage backend (no dependency on .env/settings)
    from app.media_processing import store_archive_images_as_webp
    from app.storage import LocalStorageBackend

    storage_root = (os.getenv("VS_TEST_STORAGE_ROOT") or "exports/storage_test").strip() or "exports/storage_test"
    public_base_url = (os.getenv("VS_TEST_PUBLIC_BASE_URL") or "").strip() or None
    quality = int((os.getenv("VS_TEST_WEBP_QUALITY") or "80").strip() or 80)
    max_images_env = (os.getenv("VS_TEST_MAX_IMAGES") or "").strip()
    max_images = int(max_images_env) if max_images_env else None

    storage = LocalStorageBackend(root_dir=storage_root, public_base_url=public_base_url)

    await store_archive_images_as_webp(
        archive=archive,
        storage=storage,
        namespace="vaultstream",
        quality=quality,
        max_images=max_images,
    )

    stored_images = archive.get("stored_images") or []
    print(f"  stored_images: {len(stored_images)}")

    # æ£€æŸ¥å®é™…æ–‡ä»¶æ˜¯å¦è½ç›˜
    existing = 0
    for it in stored_images[:5]:
        key = (it or {}).get("key")
        if not isinstance(key, str) or not key:
            continue
        full_path = os.path.join(storage_root, key.lstrip("/"))
        if os.path.exists(full_path):
            existing += 1
    print(f"  stored_files_exist(sample_5): {existing}/5")
    print(f"  storage_root: {storage_root}")

    # å¯¼å‡ºå¤„ç†åçš„ archiveï¼Œä¾¿äºäººå·¥æ ¸å¯¹ stored_* å­—æ®µ
    export_dir = (os.getenv("VS_EXPORT_DIR") or "exports").strip() or "exports"
    os.makedirs(export_dir, exist_ok=True)
    out_path = os.path.join(export_dir, "opus_fixture.archive.processed.json")
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(archive, f, ensure_ascii=False, indent=2)
    print(f"  ğŸ—ƒï¸  å·²å¯¼å‡º processed archiveï¼š{out_path}")


if __name__ == "__main__":
    print("ğŸ§ª VaultStream - Bç«™é€‚é…å™¨æµ‹è¯•")
    print("=" * 60)

    # è¿è¡Œæµ‹è¯•ï¼šå…ˆè·‘ç¦»çº¿ fixtureï¼Œé¿å…è¢«äº¤äº’/ç½‘ç»œå½±å“
    try:
        asyncio.run(test_opus_archive_from_curl_fixture())
    except KeyboardInterrupt:
        print("\nâš ï¸  å·²ä¸­æ–­ç¦»çº¿ fixture æµ‹è¯•")

    try:
        asyncio.run(test_archive_media_processing_local_from_fixture())
    except KeyboardInterrupt:
        print("\nâš ï¸  å·²ä¸­æ–­ storage/webp ç¦»çº¿éªŒè¯")

    try:
        asyncio.run(test_bilibili_adapter())
    except KeyboardInterrupt:
        print("\nâš ï¸  å·²ä¸­æ–­åŸºç¡€é€‚é…å™¨æµ‹è¯•")

    try:
        asyncio.run(test_real_url())
    except KeyboardInterrupt:
        print("\nâš ï¸  å·²ä¸­æ–­çœŸå® URL æµ‹è¯•")

    try:
        asyncio.run(test_opus_archive())
    except KeyboardInterrupt:
        print("\nâš ï¸  å·²ä¸­æ–­ opus å½’æ¡£æµ‹è¯•")

    print("\nâœ¨ æµ‹è¯•å®Œæˆï¼")
